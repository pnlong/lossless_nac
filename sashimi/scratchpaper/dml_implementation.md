# Actual Implementation of DML

## Implementation Progress

### Step 1: Output Head Implementation âœ“
Created `/src/models/sequence/modules/output_heads.py`:
- Implemented CategoricalHead for backward compatibility
- Added DMLHead with mixture parameter outputs
- Created factory function for head selection
- Verified linting and imports

### Step 2: Loss Implementation âœ“
Created `/src/models/sequence/loss.py`:
- Implemented both categorical and DML losses
- Added factory function for loss selection
- **Updated DML Loss to Match Original TensorFlow Implementation**:
  - âœ… **Bin Size Fix**: Changed from `2.0/num_classes` to `1.0/(num_classes-1)` to match original's `1/255` discretization
  - âœ… **log_prob_from_logits**: Added numerically stable log_softmax equivalent to original's function
  - âœ… **Log-space CDF Computation**: Implemented sophisticated log-space probability calculations using `softplus`
  - âœ… **Robust Edge Case Handling**: Added nested `torch.where` logic matching original's `tf.where` structure
  - âœ… **Fallback for Extreme Cases**: Added `log_pdf_mid` approximation for cases with very small probability mass
  - âœ… **Proper Normalization**: Used equivalent of `np.log(127.5)` as `math.log((num_classes-1)/2)`
- **Added DML Sampling Function**:
  - âœ… **sample_from_discretized_mix_logistic**: PyTorch equivalent of original's sampling code
  - âœ… **Gumbel-max Trick**: For mixture component selection (numerically stable)
  - âœ… **Logistic Sampling**: Inverse CDF sampling from selected mixture component
  - âœ… **Temperature Control**: Added temperature parameter for controlling randomness
  - âœ… **1D Adaptation**: Removed channel dependencies (coeffs) for audio sequences
- Added detailed statistics logging:
  - Average scale
  - Average mean
  - Mixture entropy

**Key Changes Made to Match Original**:

1. **Bin Size Calculation** (Line 86):
   ```python
   # Original TensorFlow: 1./255. for 8-bit
   # Our implementation: 1.0 / (num_classes - 1)
   bin_size = 1.0 / (num_classes - 1)
   ```

2. **Log Probability Computations** (Lines 103-111):
   ```python
   # Equivalent to original's log-space computations:
   log_cdf_plus = plus_in - F.softplus(plus_in)        # log prob for left edge
   log_one_minus_cdf_min = -F.softplus(min_in)        # log prob for right edge
   log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)  # center approximation
   ```

3. **Robust Edge Case Selection** (Lines 120-140):
   ```python
   # Matches original's nested tf.where structure:
   log_probs = torch.where(is_left_edge, log_cdf_plus, log_probs)
   log_probs = torch.where(is_right_edge, log_one_minus_cdf_min, log_probs)
   # Fallback for extreme cases with small cdf_delta
   log_probs = torch.where(normal_mask & (cdf_delta <= 1e-5),
                          log_pdf_mid - log_normalizer, log_probs)
   ```

4. **Mixture Weighting** (Line 143):
   ```python
   # Uses our log_prob_from_logits instead of F.log_softmax for stability
   log_probs = torch.sum(log_probs, dim=-1) + log_prob_from_logits(logit_probs)
   ```

### Step 3: Sashimi Backbone Modification âœ“
Modified `/src/models/sequence/backbones/sashimi.py`:
- Added output head configuration parameters:
  - output_head: 'categorical' (default) or 'dml'
  - n_mixtures: number of mixture components (default 10)
- Added output head initialization
- Modified d_output property to handle both types
- Updated forward/step to use output head
- Maintained backward compatibility

Key changes:
1. Added to __init__:
```python
def __init__(
    self,
    d_model,
    output_head='categorical',  # New: output head type
    n_mixtures=10,             # New: number of mixture components for DML
    ...
):
    # ... existing initialization ...
    
    # Initialize output head
    head_kwargs = {
        "n_mixtures": n_mixtures if output_head == "dml" else None,
        "n_classes": self.dataset.n_tokens if output_head == "categorical" else None,
    }
    self.output_head = get_output_head(
        output_head, 
        d_model, 
        **{k: v for k, v in head_kwargs.items() if v is not None}
    )
```

2. Modified d_output:
```python
@property
def d_output(self):
    """Output dimension depends on output head type"""
    if self.output_head_type == "categorical":
        return self.dataset.n_tokens
    elif self.output_head_type == "dml":
        return 3 * self.n_mixtures
```

3. Updated forward/step:
```python
def forward(self, x, state=None, **kwargs):
    # ... existing processing ...
    x = self.norm(x)
    x = self.output_head(x)  # Apply output head
    return x, None
```

### Next Step: AudioGenerationTask
Will create task implementation next:
- Add new task class
- Handle both output types
- Pass dataset to loss function
- Set up metrics

### Step 4: Dataset Modifications âœ“
Modified `/src/dataloaders/audio.py`:
- Added `output_head` and `n_mixtures` parameters to `QuantizedAutoregressiveAudio.init_defaults`
- Modified `d_output` property to return correct dimensions:
  - Categorical: `1 << bits` (e.g., 256 for 8-bit audio)
  - DML: `3 * n_mixtures` (e.g., 30 for 10 mixtures)
- Maintained backward compatibility with existing configs

### Step 5: Configuration Files âœ“
Created all necessary configuration files:

**Model Config** (`/configs/model/sashimi.yaml`):
- Added `output_head: categorical` (default)
- Added `n_mixtures: 10` (only used for DML)

**Loss Configs**:
- `/configs/loss/categorical.yaml`: Uses `cross_entropy`
- `/configs/loss/dml.yaml`: Uses `dml`

**Experiment Config** (`/configs/experiment/audio/sashimi-dml.yaml`):
- Complete DML training configuration
- Uses `model.output_head: dml`
- Uses `task.loss: dml`
- All other settings optimized for audio generation

### Step 6: Integration Testing âœ“
All components successfully integrated:
- âœ… BaseTask modified to handle DML losses with dataset parameter (fixed name matching)
- âœ… DML loss function registered in metrics system
- âœ… Output heads factory working correctly
- âœ… Sashimi backbone supports both head types (fixed dataset access issue)
- âœ… Dataset properly configured for both modes
- âœ… Model parameters (n_classes, n_mixtures) properly configured
- âœ… Decoder handles both tensor and dictionary inputs (DML support)
- âœ… Torchmetrics disabled for DML to prevent memory issues

**Bug Fixes**:
1. **AttributeError**: Resolved issue where Sashimi tried to access `self.dataset.n_tokens` before dataset was initialized. Fixed by:
   - Modified training script to pass `bits` from dataset config to model during instantiation
   - Model `bits` parameter defaults to 8 if not provided (for backward compatibility)
   - Calculating `n_classes = 1 << bits` internally in the model
   - Removed `bits` from model YAML config - it's automatically derived from dataset.bits
   - Ensures model and dataset configurations stay in sync

2. **TypeError: unhashable type: 'slice'**: Fixed lambda functions in SequenceDecoder that caused hashability issues and added support for dictionary inputs from DML output heads:
   ```python
   # Before (problematic):
   restrict = lambda x: x[..., -l_output:, :]
   
   # After (fixed):
   def restrict(x):
       return x[..., -l_output:, :]
   
   # Added dictionary handling:
   if isinstance(x, dict):
       # Handle DML output (dictionary of tensors)
       x = {key: restrict(tensor_val) for key, tensor_val in x.items()}
   else:
       # Handle categorical output (tensor)
       x = restrict(x)
   ```

3. **ValueError: dataset required for DML loss**: Fixed timing issue in BaseTask where the loss name was checked on the instantiated function object instead of the original config:
   ```python
   # Before (incorrect - checking after instantiation):
   if hasattr(loss, '_name_') and loss._name_ == 'dml':  # loss is functools.partial

   # After (correct - checking before instantiation):
   if isinstance(loss, str) and loss == 'dml':  # loss is still config string
       is_dml_loss = True
   ```

4. **Memory Issue (80 GiB usage)**: Fixed excessive memory usage caused by torchmetrics trying to allocate memory for categorical classes (256) when DML outputs dictionaries:
   ```python
   # Before: torchmetrics allocated memory for 256 classes
   self._tracked_torchmetrics[prefix][name] = getattr(tm, name)(
       average='macro', num_classes=self.dataset.d_output, ...)

   # After: Skip torchmetrics for DML, use dummy metrics
   if self.model.output_head_type == 'dml':
       self._tracked_torchmetrics[prefix][name] = DummyMetric()
   ```

5. **TypeError: unsupported operand type(s) for /: 'dict' and 'float'**: Fixed loss metrics trying to perform mathematical operations on DML dictionary outputs:
   ```python
   # Before: Failed when DML returned dict
   return loss_fn(x, y) / math.log(2)  # TypeError!

   # After: Extract 'loss' key from DML dictionary
   loss_output = loss_fn(x, y)
   if isinstance(loss_output, dict):
       loss_val = loss_output['loss']
   else:
       loss_val = loss_output
   return loss_val / math.log(2)
   ```

6. **ValueError: dict values cannot be logged**: Fixed PyTorch Lightning logging issue with DML dictionary outputs:
   ```python
   # Before: Tried to log dictionary directly
   metrics["loss"] = loss  # loss is a dict for DML

   # After: Extract scalar values for logging
   if isinstance(loss, dict):
       metrics["loss"] = loss["loss"]  # Main loss value
       metrics["avg_scale"] = loss["avg_scale"]  # DML monitoring
       metrics["avg_mean"] = loss["avg_mean"]    # DML monitoring
       metrics["mixture_entropy"] = loss["mixture_entropy"]  # DML monitoring
   else:
       metrics["loss"] = loss
   ```

### Usage Examples:

**Default categorical training** (backward compatible):
```bash
python train.py experiment=audio/sashimi-musdb18mono
```

**DML training** (loss automatically inferred):
```bash
python train.py experiment=audio/sashimi-dml-musdb18mono
# Output: ðŸš€ DML MODE ACTIVATED! (with detailed info)
```

**DML training with YouTube Mix**:
```bash
python train.py experiment=audio/sashimi-dml-youtubemix
# Note: Uses shorter sequences (65536 â†’ 12288) to reduce memory usage
```

**Override parameters**:
```bash
python train.py experiment=audio/sashimi-dml-musdb18mono model.n_mixtures=20
```

### Key Improvements:

- âœ… **Automatic Loss Inference**: When `model.output_head: dml`, the task automatically uses DML loss
- âœ… **Dictionary Input Handling**: All metrics functions now handle DML dictionary outputs
- âœ… **Proper Logging Support**: DML dictionary outputs properly unpacked for PyTorch Lightning logging
- âœ… **DML Monitoring Metrics**: Additional metrics (avg_scale, avg_mean, mixture_entropy) logged for DML
- âœ… **DML Sampling Support**: Added `sample_from_discretized_mix_logistic` for generation/inference
- âœ… **Temperature Control**: Sampling supports temperature parameter for controlling randomness
- âœ… **Single Configuration**: Only need to set `output_head: dml` in model config
- âœ… **Backward Compatible**: Existing categorical configs work unchanged
- âœ… **Clear DML Activation Logging**: Training shows ðŸš€ DML MODE ACTIVATED! with details

### Step 7: Loss Function Refinement âœ“
**Completed**: Updated DML loss implementation to closely match original TensorFlow code:
- Fixed bin size discretization to match original's `1/255` approach
- Implemented log-space probability computations using `softplus`
- Added robust edge case handling with fallback approximations
- Added `log_prob_from_logits` for numerical stability of mixture weights
- All changes maintain backward compatibility and same output format

**Comparison with Original Implementation**:
- âœ… Bin size: `1.0/(num_classes-1)` matches original's discretization
- âœ… CDF computation: Log-space with `softplus` matches original's approach
- âœ… Edge cases: Nested `where` conditions with `log_pdf_mid` fallback
- âœ… Mixture weighting: Uses stable `log_prob_from_logits` equivalent
- âœ… Normalization: Proper log-space handling throughout

### Step 8: DML Sampling Implementation âœ“
**Completed**: Added complete sampling support for DML generation:
- âœ… PyTorch equivalent of original TensorFlow `sample_from_discretized_mix_logistic`
- âœ… Gumbel-max trick for mixture component selection
- âœ… Inverse CDF sampling from logistic distribution
- âœ… Temperature control for generation diversity
- âœ… Adapted for 1D audio sequences (no channel dependencies)
- âœ… **Fixed shape mismatch bug** in loss function mixture weighting
- âœ… **Verified both training and sampling work correctly**

**Usage Examples**:

**Training (Loss Function)**:
```python
from src.models.sequence.loss import discretized_mix_logistic_loss

result = discretized_mix_logistic_loss(predictions, targets, dataset)
loss = result['loss']  # Main loss value
```

**Generation (Sampling)**:
```python
from src.models.sequence.loss import sample_from_discretized_mix_logistic

# During inference/generation:
predictions = model(x)  # Get DML predictions
num_classes = 256  # For 8-bit audio
temperature = 0.8  # Lower = more deterministic, higher = more diverse

# Sample new audio values
samples = sample_from_discretized_mix_logistic(
    predictions=predictions,
    num_classes=num_classes,
    temperature=temperature
)
```

**Temperature Control**:
- `temperature=0.1`: Conservative, more deterministic sampling
- `temperature=1.0`: Balanced diversity (default)
- `temperature=2.0`: High diversity, more creative sampling

### Step 9: DML Generation Pipeline Integration âœ“
**Completed**: Full DML support integrated into inference pipeline:

**Modified Files**:
- âœ… **`generate.py`**: Added DML-aware sampling in generation loop
- âœ… **`train.py`**: Fixed DML dictionary logging issue in training_step
- âœ… **Decoder already supported**: `SequenceDecoder.step()` handles DML dictionary outputs

**Bug Fixes**:
- âœ… **Dictionary Logging Error**: Fixed `ValueError: dict values cannot be logged` by extracting scalar loss from DML dictionary in `training_step`
- âœ… **Pretrained Model Variable**: Fixed undefined `pretrained_model` variable in pretrained loading logic

**Key Integration Points**:

1. **DML Detection** (Line 89 in generate.py):
   ```python
   is_dml = isinstance(y_t, dict) and all(k in y_t for k in ['logit_probs', 'means', 'log_scales'])
   ```

2. **Conditional Sampling** (Lines 91-118):
   ```python
   if is_dml:
       # Use DML sampling
       y_t = sample_from_discretized_mix_logistic(predictions=y_t, num_classes=num_classes, temperature=tau)
   else:
       # Use categorical sampling
       probs = F.softmax(y_t, dim=-1)
       y_t = Categorical(logits=y_t/tau).sample()
   ```

3. **Log Probability Handling** (Lines 128-142):
   - Categorical: Exact logprob calculation
   - DML: Placeholder (approximation would be expensive during generation)

**Usage for DML Generation**:
```bash
# Generate with DML model
python generate.py experiment=audio/sashimi-dml-musdb18mono \
    temp=0.8 \        # Temperature for mixture component selection
    l_sample=16000 \  # 1 second of audio at 16kHz
    n_samples=10
```

**Decoder Integration**:
- âœ… `SequenceDecoder.step()` already handles DML dictionary outputs correctly
- âœ… No modifications needed - returns DML predictions as-is for sampling
- âœ… Categorical outputs still get `output_transform` applied

### Remaining Tasks:
1. **End-to-end testing** - Run actual training to verify improved functionality
2. **Performance validation** - Compare improved DML vs categorical performance
3. **Documentation** - Update README with DML usage instructions